{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import os\n",
    "\n",
    "# 构造 FAISS 向量数据库的路径\n",
    "current_dir = os.getcwd()\n",
    "faiss_index_path = os.path.join(current_dir, \"..\", \"data\", \"vec_databases\", \"vecdb.faiss\")\n",
    "\n",
    "# 加载本地 FAISS 向量数据库\n",
    "vec_db = faiss.read_index(faiss_index_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_db.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of vectors in the database: 2\n",
      "All vectors removed from the database\n",
      "Empty FAISS index saved to: c:\\Users\\shann\\Desktop\\SE\\attendance_system\\scripts\\..\\data\\vec_databases\\vecdb.faiss\n"
     ]
    }
   ],
   "source": [
    "# Get the original number of vectors in the database\n",
    "original_count = vec_db.ntotal\n",
    "print(f\"Current number of vectors in the database: {original_count}\")\n",
    "\n",
    "# Remove all vectors from the index\n",
    "vec_db.reset()\n",
    "print(f\"All vectors removed from the database\")\n",
    "\n",
    "# Save the empty index back to disk\n",
    "faiss.write_index(vec_db, faiss_index_path)\n",
    "print(f\"Empty FAISS index saved to: {faiss_index_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append(os.path.join(current_dir, \"..\"))\n",
    "from src.outer_lab.facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "# Load YOLOv8 face detection model\n",
    "face_detector = YOLO(\"train_model/train2/weights/best.pt\")\n",
    "\n",
    "# Load face feature extractor (assuming you're using a model like FaceNet or similar)\n",
    "# This is a placeholder - replace with your actual feature extraction model\n",
    "feature_extractor = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# Function to process image and find matches\n",
    "def process_image_and_find_matches(image_path, top_k=5):\n",
    "    # Load and process the image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        return []\n",
    "    \n",
    "    # Detect faces using YOLOv8\n",
    "    results = face_detector(img)\n",
    "    \n",
    "    matches = []\n",
    "    \n",
    "    # Process each detected face\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            # Get face bounding box coordinates\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "            \n",
    "            # Extract the face\n",
    "            face = img[y1:y2, x1:x2]\n",
    "            \n",
    "            # Preprocess face for feature extraction\n",
    "            face_pil = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
    "            face_pil = face_pil.resize((160, 160))  # Adjust size according to your feature extractor\n",
    "            face_tensor = torch.from_numpy(np.array(face_pil)).permute(2, 0, 1).float().unsqueeze(0) / 255.0\n",
    "            \n",
    "            # Extract face features\n",
    "            with torch.no_grad():\n",
    "                face_embedding = feature_extractor(face_tensor)\n",
    "                face_embedding = face_embedding.cpu().numpy()\n",
    "                face_embedding = face_embedding / np.linalg.norm(face_embedding)  # Normalize the embedding\n",
    "                \n",
    "            return face_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 544x640 1 face, 17.8ms\n",
      "Speed: 7.5ms preprocess, 17.8ms inference, 4.5ms postprocess per image at shape (1, 3, 544, 640)\n"
     ]
    }
   ],
   "source": [
    "image_path = \"../data/photos/face1.png\"\n",
    "feature = process_image_and_find_matches(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0.073315,    0.025664, -3.4028e+38, -3.4028e+38, -3.4028e+38]], dtype=float32),\n",
       " array([[2008, 2007,   -1,   -1,   -1]], dtype=int64))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(feature)\n",
    "res = index.search(feature, 5)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
